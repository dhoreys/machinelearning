# Goal : Predict the manner in which exercises were performed
Background: Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

- exactly according to the specification        (Class A)
- throwing the elbows to the front              (Class B)
- lifting the dumbbell only halfway             (Class C)
- lowering the dumbbell only halfway            (Class D)
- throwing the hips to the front                (Class E)

We will identify how the test objects (participarnts) performed exercises and categorize the findings in one of these 5 levels.

Data is sourced from http://groupware.les.inf.puc-rio.br/har

``` {r}

setInternet2(use = TRUE)
ftest   <- 'pml-test.csv'
testurl <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

ftrain   <- 'pml-training.csv'
trainurl <- 'http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'

if (!file.exists(ftest)){
        download.file(testurl, ftest)
}
if (!file.exists(ftrain)){
        download.file(trainurl, ftrain)
}

trainingset <- read.csv(ftrain, header=TRUE, sep=",", as.is = TRUE, na.strings=c("NA","", "#DIV/0!"))
testingset  <- read.csv(ftest, header=TRUE, sep=",", as.is = TRUE, na.strings=c("NA","", "#DIV/0!"))

```


```{r}
#check dimensions
dim(trainingset) #shows 19622 rows and 160 columns
dim(testingset)  #shows 20    rows and 160 columns
```

##### Exploratoy data analysis
You can clearly see different levels. No outliers or erroneous data.

```{r}
# this will give you some idea about diff factors and bar plot
summary(factor(trainingset$classe))
barplot(table(trainingset$classe), main = "Explore output variable  - classe", xlab="Level of Exercise", ylab="# of observations", legend.text= c("A - exactly according to the specification", "B - throwing the elbows to the front", "C - lifting the dumbbell only halfway", "D - lowering the dumbbell only halfway", "E - throwing the hips to the front"), args.legend = list(x = "topright") )

```

##### Load libraries, set seed, remove unwanted columns

```{r, echo=FALSE, message=FALSE}
#load library and set seed
library(caret)
library(randomForest)
library(ggplot2)
library(lattice)
library(rpart)
library(rpart.plot)
library(corrplot)
```

```{r}
set.seed(121)
#removing unwanted columns
trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]

#removing columns with all the missing values
trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]

#check the new sets, note that the columns dropped to 53
dim(trainingset) #shows 19622 rows and 53 columns
dim(testingset)  #shows 20    rows and 53 columns
```


##### Partitioning the training data set to allow cross-validation

```{r}
subsamples <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
subTraining <- trainingset[subsamples, ] 
subTesting <- trainingset[-subsamples, ]

# Confirm that there are still 53 columns!
dim(subTraining)
dim(subTesting)
#head(subTraining)
```


##### Find correlated predictors using principle components analysis
Dark blue or dark red columns are highly correlated.
Remove highly correlated predictors and replace them with weighted combination of predictors. 
The new dataset will be used for rest of the exercise.

```{r}
# since column 53 is classe, we will find out correlation of rest of the columns and plot it.
M1 <- abs(cor(subTraining[,-53]))
#diag(M1) <- 0
#which(M1 > 0.8, arr.ind=T)
corrplot(M1, order = "FPC", method = "color", type = "lower", tl.cex = 0.8, tl.col = rgb(0, 0, 0))

preProc <- preProcess(subTraining[, -53], method = "pca", thresh = 0.99)
trainPC <- predict(preProc, subTraining[, -53])
valid_testPC <- predict(preProc, subTesting[, -53])
```


##### First prediction model: Using Decision Tree

```{r}
model1 <- rpart(subTraining$classe ~ ., data=trainPC, method="class")
prediction1 <- predict(model1, valid_testPC, type = "class")
rpart.plot(model1, main="Classification Tree", extra=0, under=TRUE, faclen=0)

# Test results on our subTesting data set:
confusionMatrix(prediction1, subTesting$classe)

```

##### Second prediction model: Using Random Forest

```{r}
model2 <- randomForest(as.factor(subTraining$classe) ~. , data=trainPC, trControl = trainControl(method = "cv", number = 4), importance=TRUE, ntree=20)

prediction2 <- predict(model2, valid_testPC)
confusionMatrix(prediction2, subTesting$classe)

```

##### Decision to choose final model
Accuracy for Random Forest Model is 0.9684 and 95% CI : (0.9631, 0.9731)
Accuracy for Decions Tree Model  is 0.5253 and 95% CI : (0.5112, 0.5393)
Obviously, random forest model is accurate and we choose this model.

##### Importance of variables
Higher the MeanDecreaseAccuracy means how much more helpful the predictor is in reducing classifying error (e.g. from LHS graph below, PC35, PC11 and PC2). Also, listing variable importance for your reference under the graph.

Higher the MeanDecreaseGini means the particular predictor plays a greater role in partitioning the data into defined classes. (e.g. from RHS graph below, PC8, pc35 and pc5 have higher MeanDecreaseGini and are impoartant).


```{r}
varImpPlot (model2, sort=TRUE)
varImp(model2)
```

##### Cross-validation and out of sample error
Note higher accuracy of our model (96.88%) and small out of sample error based on cross-validation dataset (3.12%).

```{r}
prediction3 <- predict(model2, valid_testPC)
confus <- confusionMatrix(subTesting$classe, prediction3)
confus$table

# model accuracy for testing data set
accur <- postResample(subTesting$classe, prediction3)
modAccuracy <- accur[[1]]
modAccuracy

out_of_sample_err <- 1 - modAccuracy
out_of_sample_err
```

##### Final tests on the original test sample provided by the teacher

There are 20 test cases in the original test set. We are writing the output in separate file as requested in the project.

```{r}
# removing the last column Classe and applying the preprocessing
testPC <- predict(preProc, testingset[, -53])
predictfinal <- predict(model2, testPC)
predictfinal

#output to files as mentioned in the proj description
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
} 

pml_write_files(predictfinal)
```